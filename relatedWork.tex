\section{Related Work}
\label{sec:relWork}

Much of the research on keyword extraction has focused on leveraging
statistical properties of a corpus or single document. Term
frequency-inverse document frequency (TF-IDF) is a widely used method
that weights phrases in a document proportionally to the number of
times they appear in that document, and inversely proportionally to
the number of documents in which the phrases appear at least once
\cite{mann2008}. Statistical methods that work on a single document
have also been proposed, such as selecting keywords that co-occur in
sentences with frequent words unevenly \cite{matsuo2004keyword}. The
TextRank system uses sentence co-occurrence data in a graph-based
approach, by forming a node for each candidate phrase and an edge
between two phrases if they appear together in a sentence. The system
then runs PageRank over the induced graph to select keywords
\cite{mihalcea2004textrank}. An approach that uses graphs for topic
clustering was presented in \cite{ohsawa1998}.  Other methods have
combined linguistic and statistical properties of the document by
first filtering the set of possible keywords (e.g. only considering
noun phrases or adjectives) and then applying frequency-based methods
\cite{rose2010automatic}.

The approaches outlined so far all take an unsupervised approach to
keyword extraction. There has also been work on supervised approaches,
which classify each phrase by whether it should be a keyword or
not. Turney trained a decision tree on different annotated corpora to
choose keywords based on features such as the length of the phrase and
whether it is a proper noun \cite{turney1999learning}. Hulth takes a
related approach, and finds that adding part-of-speech tags as a
feature leads to improved results \cite{hulth2003improved}. The major
downside of supervised learning is the expense associated with
labeling data, as many types of documents will require a person with
domain knowledge to choose keywords. As a result many of these
supervised systems use academic journals with author-provided keywords
as datasets. Supervised approaches are not an option in our use case.

Discerning important terms in the specific setting of instructional
videos has also been investigated. This task differs from keyword
extraction from a large heterogeneous document collection because of
the sequential nature of the videos, the fact that they will be about
closely related topics, and the additional audio-visual
component. Methods have been designed to combine the statistical
properties of the lecture transcripts with cues from the lecture
videos, such as the introduction of a new speaker, and evaluated on
governmental instruction videos \cite{park2006extracting}. This work
differs from the work presented here along several dimensions. The
human raters were constrained to use keywords that were pre-selected
by the authors' baseline keyword extraction algorithm. Furthermore,
rather than introducing Wikipedia for extraction support the cited
work relies on video features. The authors report percent-agreement
between their algorithm and the gold set, which can be misleading
compared to indicators such as Cohen's Kappa. Nonetheless
\cite{park2006extracting} is very much in the spirit of our
investigation. 

Keyword extraction from Khan Academy lecture videos has been
experimented with, using statistical properties of the lecture
transcripts \cite{6337092}.

Finally, there has been research into using Wikipedia's external knowledge for natural language processing tasks such as clustering documents \cite{hu2009exploiting} and computing semantic similarity between words \cite{milne2007computing}.
