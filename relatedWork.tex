\section{Related Work}
\label{sec:relWork}

Much of the research on keyword extraction has focused on leveraging statistical properties of a corpus or single document. Term frequency-inverse document frequency (tf-idf) is a widely used method that weights phrases proportionally to the number of times they appear and inversely proportionally to the number of documents they appear in \cite{mann2008}. Statistical methods that work on a single document have also been proposed, such as selecting keywords that co-occur in sentences with frequent words unevenly \cite{matsuo2004keyword}. The TextRank system uses sentence co-occurrence data in a graph-based approach, by forming an node for each candidate phrase and an edge between two phrases if they appear together in a sentence. The system then runs PageRank over the induced graph to select keywords \cite{mihalcea2004textrank}. Other methods have combined linguistic and statistical properties of the document by first filtering the set of possible keywords (e.g. only consider noun phrases or adjectives) and then applying frequency based methods \cite{rose2010automatic}.

The approaches outlined so far all take an unsupervised approach to keyword extraction. There has also been work on supervised approaches, which classify each phrase by whether it should be a keyword or not. Turney trained a decision tree on different annotated corpora to choose keywords based on features such as the length of the phrase and whether it is a proper noun \cite{turney1999learning}. Hulth takes a related approach, and finds that adding part-of-speech tags as a feature leads to improved results \cite{hulth2003improved}. The major downside of supervised learning is the expense associated with labeling data, as many types of documents will require a person with domain knowledge to choose keywords. As a result many of these supervised systems use academic journals with author-provided keywords as datasets.

Discerning important terms in the specific setting of instructional videos has also been investigated. This task differs from keyword extraction from a large heterogeneous document collection because of the sequential nature of the videos, the fact that they will be about closely related topics, and the additional audio-visual component. Methods have been designed to combine the statistical properties of the lecture transcripts with cues from the lecture videos, such as the introduction of a new speaker, and evaluated on governmental instruction videos \cite{park2006extracting}. Keyword extraction from Khan Academy lecture videos has been experimented with, using statistical properties of the lecture transcripts \cite{6337092}.

Finally, there has been research into using Wikipedia's external knowledge for natural language processing tasks such as clustering documents \cite{hu2009exploiting} and computing semantic similarity between words \cite{milne2007computing}.
