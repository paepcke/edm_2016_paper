\section{Preparation of Gold Index}
\label{sec:gold}

% Describe how we had the index prepared: Paid workers, the course,
% instructions they received. Some stats: average number of index terms
% they came up with, inter-rater reliability.

In order to evaluate our algorithms, we prepared a gold standard index
of terms extracted from an introductory databases course by three paid
human indexers. Each was presented with all ~90 course closed caption
video transcripts, and was asked to work through each file in the
order the videos were presented in class. Videos were usually around
10 minutes long.

From each line in a file the indexers were asked to select as many
keywords and phrases as they felt should appear in their
index. Two of the indexers only used words or phrases that
appeared in the text. One expert indexer added synonyms.

We used indexers' selected keywords and phrases in our
investigation\footnote{From here on we use the term `phrase' to mean
  n-grams of any length, including one, unless the distinction is
  significant.}. Additionally we asked indexers for two more pieces of
information that we did not use, but which are included in our public
copies of the indexers' results.

First, we asked indexers also to mark lines in which a particular
phrase for the index appeared in the context of the primary
introduction to the phrase's underlying concept.  Second, for each
lecture file, indexers ranked the top five most important phrases from
that lecture. We allowed inclusion of fewer than five phrases.

One participant had taken the database course being indexed. A second
indexer had taken at least one database course in another institution,
and the third was a college-educated individual in a non-technical
field.

On average indexers selected about 8 phrases per video.  We evaluated
agreement between the indexers' output using Fleiss' Kappa, which
generalizes Cohen's Kappa to settings with more than two annotators
\cite{fleiss1971measuring}. The $\kappa$ is a measure of inter-rater
agreement, with range $[-1, 1]$, where 1 means the raters are in
complete agreement, -1 complete disagreement, and 0 the agreement
expected by chance.

For our algorithms we formulate the indexing problem as a binary
classification task, where the two categories are {\em in-index} and
{\em not-in-index}. The algorithms' task was to classify phrases in
each lecture into these two buckets.

%% Specifically, we have the algorithms classify phrases that are either
%% 1-grams or have been tagged as an {\em in-index} phrase by at least one
%% rater. For example, in the sentence `One of them is what's called
%% the inner join on a condition', where `inner join' has been marked as
%% a keyword, the algorithms will be evaluated on their classification
%% of `inner join' and `condition', but not on the phrase `one of'.
% Given a set of raters and a collection of lectures $\ell_1, \ldots,
% \ell_n$, where each lecture contains phrases $p_1, \ldots, p_n \in
% \ell_i$, we define the set of examples as the set of lecture-phrase
% pairs $(\ell, p)$ such that $p$ appears in $\ell$, and $p$ is either a
% 1-gram or $(\ell, p)$ was tagged as an {\em in-index} phrase by at
% least one rater.

When not comparing against each of the three indexers individually, we
combined the work of all three into a single index by computing the
lecture-by-lecture three-way union. Several other approaches for
combining the three ground truth examples can be formulated. We
selected the union because it was the most licentious for the
algorithms. However, when comparing algorithm results to individual
indexers the respective indexer's actual work was used.

We computed a $\kappa$ of 0.325 between the indexes produced by the
three human raters. This $\kappa$ is evidence of significant
differences between the decisions of the three indexers. The
non-expert was much more prolific in choosing {\em in-index} phrases
than the two experts. But even the two experts frequently made
different choices.

We consider this $\kappa$ the upper bound on the performance we could
reasonably expect from any indexing algorithm. The largest pairwise
Cohen's $\kappa$ between raters was 0.336 and the lowest was 0.309
(note that Fleiss' Kappa is not generally the average of the pairwise
Cohen's Kappas).
