\subsection{Summary of Results}
\label{sec:sum}

% Here is where the comparisons between the two/three experimental
% results happens.


\begin{figure*}[!ht]
\caption{}
\label{fig:main_result}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
\hline
TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
\hline
TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
\hline
Document Boosting & 0.180 & 0.199 & \textbf{0.974} & \textbf{0.901} \\
\hline
Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
\hline
Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
\hline
Phrase Boosting N-Grams & \textbf{0.191} & \textbf{0.213} & \textbf{0.974} & 0.899 \\
\hline
\end{tabular}
\end{figure*}

As stated in section \ref{sec:gold}, we computed agreement between humans using Fleiss' Kappa. We evaluate each algorithm by computing Cohen's Kappa agreement between the algorithm and the union of the lecture-phrase pairs in the gold index, i.e. a lecture-phrase pair is classified as ``keyword'' if any human rater classified it as a keyword, and classified as ``not keyword'' otherwise. When using a single metric for agreement, such as Cohen's Kappa, paradoxes can arise where one category is much more prevalent than another and the chance-correction term in the agreement metric overcompensates, leading to low values of agreement \cite{feinstein1990high}. In our case, because there are many more possible phrases than keywords in a lecture, most phrases are calculated as ``not keyword''. To resolve this paradox, we report three additional metrics: agreement on keywords ($P_{\text{pos}}$), agreement on other phrases ($P_{\text{neg}}$) \cite{cicchetti1990high}, and prevalence-adjusted bias-adjusted kappa (PABAK) \cite{byrt1993bias}.

These three metrics are defined in terms of a concordance table. Let $a$ be the number of terms both humans and the algorithms marked as ``keyword'', ``d'' be the number of terms both marked as ``not keyword'', and $n = (a + d) / 2$ be their average. Let $f_1$ be the total number of terms humans marked as ``keyword'', $f_2$ be the total number of terms humans marked as ``not keyword'', $g_1$ be the total number of terms the algorithm marked as ``keyword'', $g_2$ be the total number of terms the algorithm marked as ``not keyword'', and $N$ be the total number of possible phrases. Then, positive agreement is the number of terms both marked as ``keyword'' over the average number of terms marked as ``keyword''
\begin{equation*}
P_{\text{pos}} \coloneqq \frac{a}{\frac{f_1 + g_1}{2}}
\end{equation*}
negative agreement is the number of terms both marked as ``not keyword'' over the average number of terms marked as ``not keyword''
\begin{equation*}
P_{\text{neg}} \coloneqq \frac{d}{\frac{f_2 + g_2}{2}}
\end{equation*}
and PABAK is defined as
\begin{equation*}
\mathrm{PABAK} = \frac{(2n / N) - 0.5}{1 - 0.5}
\end{equation*}

Kappa values do not have a universally agreed upon interpretation, but values in the range we observe (about 0.15 to 0.3) have been interpreted as indicating ``slight'' to ``fair'' agreement. The fact that agreement between humans is only 0.336, and the lowest pairwise Kappa between humans was 0.309 suggests that the keyword extraction task is inherently subjective, and there are mutiple valid interpretations of what phrases are important enough to be attached to a document.

The vanilla TF-IDF algorithm was already able to achieve reasonable performance, with respect to the human annotators. Limiting the candidate set to adjective-noun chunks drastically hurt the performance of the algorithm, suggesting that many important phrases do not fit this linguistic pattern, and the restriction is too severe. Document Boosting and Phrase Boosting, the two algorithms that incorporated external knowledge, were able to make improvements on the basic algorithm. Phrase Boosting with the TF-IDF scores of all candidate keywords was slightly better than TF-IDF, and only boosting longer phrases (Phrase Boosting N-Grams) was able to improve further. We can see that negative agreement was high for all algorithms, suggesting that it is relatively easy to identify phrases that shouldn't be keywords. Positive agreement was lower, implying it is indeed difficult to identify a small number of phrases to summarize the document, given that there are a large number of possible phrases, and there can be legitimate disagreement on what phrases are most important. Note that PABAK is not linearly related to Cohen's Kappa, $P_{\text{pos}}$, and $P_{\text{neg}}$, which explains how the Document Boosting algorithm can have a higher PABAK despite having a lower $P_{\text{pos}}$ and Cohen's Kappa.
