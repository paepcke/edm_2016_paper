\subsection{Summary of Results}
\label{sec:sum}

% Here is where the comparisons between the two/three experimental
% results happens.


\begin{figure*}[!ht]
\caption{}
\label{fig:main_result}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
\hline
TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
\hline
TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
\hline
Document Boosting & 0.180 & 0.199 & 0.974 & 0.901 \\
\hline
Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
\hline
Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
\hline
Phrase Boosting N-Grams & \textbf{0.204} & \textbf{0.224} & \textbf{0.975} & \textbf{0.902} \\
\hline
\end{tabular}
\end{figure*}

As stated in Section~\ref{sec:gold}, we computed agreement between
humans using Fleiss' Kappa. We evaluated each algorithm by computing
Cohen's Kappa agreement between the algorithm and the gold set
unified from the three human indexes as described in
Section~\ref{sec:gold}: the union of the lecture-phrase pairs in the
gold indexes. That is a lecture-phrase pair is classified as {\em
  in-index} if any human indexer classified it as {\em in-index}, and
classified as {\em not-in-index} otherwise. When using a single metric
for agreement, such as Cohen's Kappa, paradoxes can arise when one
category is much more prevalent than another and the chance-correction
term in the agreement metric overcompensates. This effect leads to
inappropriately low values of $\kappa$ for high inter-rater agreement
\cite{feinstein1990high}.

In our case, there are many more {\em not-in-index} phrases than
phrases destined for the index. To illuminate the above-mentioned
inappropriate $\kappa$ values, we report three additional metrics:
agreement on {\em in-index} decisions ($P_{\text{pos}}$), agreement on
{\em not-in-index} decisions ($P_{\text{neg}}$)
\cite{cicchetti1990high}, and prevalence-adjusted bias-adjusted
$\kappa$ (PABAK) \cite{byrt1993bias}. Because the algorithms produce a
ranking of candidate phrases, we produce a binary classification by
choosing a threshold above which candidates are labeled as {\em
  in-index}. In our reported results, we use the average number of
keywords per lecture labeled by humans as the threshold.

These three metrics are defined in terms of a concordance table with
two raters (the composite-human and the algorithm) and two
categories ({\em in-index} and {\em not-in-index})

\begin{figure}[h]
\caption{The concordance table used to calculate $P_{\text{pos}}$ and $P_{\text{neg}}$. }
\label{fig:concordance_1}
\begin{tabular}{l || c c c}
& {\em in-index} & {\em not-in-index} & Total \\
\hline \hline
Keyword & $a$ & $b$ & $f_1$ \\
Not Keyword & $c$ & $d$ & $f_2$ \\
Total & $g_1$ & $g_2$ & $N$ \\
\end{tabular}
\end{figure}

Positive agreement is the number of terms both marked as {\em
  in-index} over the average number of terms marked as {\em in-index} and
negative agreement is the number of terms both marked as {\em not
  in-index} over the average number of terms marked as {\em not
  in-index}
\begin{equation*}
P_{\text{pos}} \coloneqq \frac{a}{\frac{f_1 + g_1}{2}}
 \qquad P_{\text{neg}} \coloneqq \frac{d}{\frac{f_2 + g_2}{2}}
\end{equation*}

PABAK is defined as the Kappa on a modified concordance table where $a$ and $d$ are replaced with their average and $c$ and $d$ are replaced with their average

\begin{figure}[h]
\caption{The concordance table used to calculate PABAK. $a$, $b$, $c$, and $d$ are as defined in Figure \ref{fig:concordance_1}}
\begin{tabular}{l || c c }
& {\em in-index} & {\em not-in-index} \\
\hline \hline
{\em in-index} & $(a + d) / 2$ & $(b + c) / 2$ \\
{\em not-in-index} & $(b + c) / 2$ & $(a + d) / 2$ \\
\end{tabular}
\end{figure}

Kappa values do not have a universally agreed upon interpretation, but
values in the range we observe (about 0.15 to 0.3) have been
interpreted as indicating ``slight'' to ``fair'' agreement. The fact
that agreement between humans is only 0.336, and the lowest pairwise
Kappa between humans was 0.309 suggests that the phrase extraction
task is inherently subjective, and there are multiple valid
interpretations of what phrases are important enough to be included in
the index.

The plain TF-IDF algorithm was already able to achieve reasonable
performance, with respect to the human annotators. Limiting the
candidate set to adjective-noun chunks drastically hurt the
performance of the algorithm, suggesting that many important phrases
do not fit this linguistic pattern, and the restriction is too
severe. Document Boosting and Phrase Boosting, the two algorithms that
incorporated external knowledge, were able to make improvements on the
basic algorithm. Phrase Boosting with the TF-IDF scores of all
candidate phrases was slightly better than TF-IDF, and only boosting
longer phrases (Phrase Boosting N-Grams) was able to improve
further. We can see that negative agreement was high for all
algorithms, suggesting that it is relatively easy to identify phrases
that should not be indexed. Positive agreement was lower, implying it is indeed difficult to identify a small number of phrases to summarize the document, given that there are a large number of possible phrases, and there can be legitimate disagreement on what phrases are most important. Note that PABAK is not linearly related to Cohen's Kappa, $P_{\text{pos}}$, and $P_{\text{neg}}$, which explains how the Document Boosting algorithm can have a higher PABAK despite having a lower $P_{\text{pos}}$ and Cohen's Kappa.

Interestingly, the algorithms seemed to be significantly closer to the
indexer who took at least one database course in another institution.
For example, the Document Boosting algorithm had a $\kappa$ of 0.240 and
a $P_{\text{pos}}$ of 0.247 when compared to this indexer, while the
$\kappa$s compared to other indexers were 0.146 and 0.191, and the
$P_{\text{pos}}$ values were 0.147 and 0.147.
