\subsection{Results}
\label{sec:sum}

% Here is where the comparisons between the two/three experimental
% results happens.


% \begin{figure*}
% \caption{}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
% \hline
% Document Boosting & 0.180 & 0.199 & 0.974 & 0.901 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
% \hline
% Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.204} & \textbf{0.224} & \textbf{0.975} & \textbf{0.902} \\
% \hline
% \end{tabular}
% \end{figure*}

% \begin{figure*}[!ht]
% \caption{Result without the third rater}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.205 & 0.233 & 0.971 & 0.889 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.079 & 0.118 & 0.961 & 0.850 \\
% \hline
% Document Boosting & 0.209 & 0.234 & 0.973 & 0.895 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.142 & 0.173 & 0.968 & 0.876 \\
% \hline
% Phrase Boosting & 0.204 & 0.234 & 0.970 & 0.883 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.237} & \textbf{0.262} & \textbf{0.974} & \textbf{0.899} \\
% \hline
% \end{tabular}
% \end{figure*}

% \begin{figure*}[!ht]
% \caption{Result with only the top 5}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.227 & 0.241 & 0.986 & 0.946 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.108 & 0.125 & 0.982 & 0.931 \\
% \hline
% Document Boosting & 0.237 & 0.250 & 0.988 & 0.951 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.148 & 0.163 & 0.985 & 0.942 \\
% \hline
% Phrase Boosting & 0.223 & 0.237 & 0.986 & 0.945 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.253} & \textbf{0.265} & \textbf{0.987} & \textbf{0.950} \\
% \hline
% \end{tabular}
% \end{figure*}

We evaluated each algorithm by computing
Cohen's Kappa agreement between the algorithm and the gold set
unified from two (one of the human indexes was not included in evaluation
because they sometimes used words that did not appear in the lecture) of the human indexes as described in
Section~\ref{sec:gold}. The intuition is a measure of inter-rater reliability,
such as the Kappa, measures how closely the algorithmic index agrees with the
human indexes.

%
% This problem arises in our context because there are many more {\em not-in-index} phrases than
% phrases destined for the index. To help deconstruct the above-mentioned
% inappropriate $\kappa$ values, we report three additional metrics:
% agreement on {\em in-index} decisions ($P_{\text{pos}}$), agreement on
% {\em not-in-index} decisions ($P_{\text{neg}}$)
% \cite{cicchetti1990high}, and prevalence-adjusted bias-adjusted
% $\kappa$ (PABAK) \cite{byrt1993bias}. Because the algorithms produce a
% ranking of candidate phrases, we produce a binary classification by
% choosing a threshold above which candidates are labeled as {\em
%   in-index}. In our reported results, we use the average number of
% keywords per lecture labeled by humans as the threshold.
%
% These three metrics are defined in terms of a concordance table with
% two raters (the composite-human and the algorithm) and two
% categories ({\em in-index} and {\em not-in-index}), as shown in
% Figure~\ref{fig:concordance_1} and Figure~\ref{fig:pabak}.

% \begin{figure}[h]
% \caption{The concordance table used to calculate $P_{\text{pos}}$ and $P_{\text{neg}}$. }
% \label{fig:concordance_1}
% \begin{tabular}{l || c c c}
% & {\em in-index} & {\em not-in-index} & Total \\
% \hline \hline
% Keyword & $a$ & $b$ & $f_1$ \\
% Not Keyword & $c$ & $d$ & $f_2$ \\
% Total & $g_1$ & $g_2$ & $N$ \\
% \end{tabular}
% \end{figure}

% Positive agreement is the number of terms both indexers marked as {\em
%   in-index} over the average number of terms marked as {\em in-index}, and
% negative agreement is the number of terms both indexers marked as {\em not
%   in-index} over the average number of terms marked as {\em not
%   in-index}:
% \begin{equation*}
% P_{\text{pos}} \coloneqq \frac{a}{\frac{f_1 + g_1}{2}}
%  \qquad P_{\text{neg}} \coloneqq \frac{d}{\frac{f_2 + g_2}{2}}
% \end{equation*}
%
% PABAK is defined as the Kappa on a modified concordance table where $a$ and $d$ are replaced with their average, and $c$ and $d$ are replaced with their average.
%
% \begin{figure}[h]
% \caption{The concordance table used to calculate PABAK. $a$, $b$, $c$, and $d$ are as defined in Figure \ref{fig:concordance_1}}
% \label{fig:pabak}
% \begin{tabular}{l || c c }
% & {\em in-index} & {\em not-in-index} \\
% \hline \hline
% {\em in-index} & $(a + d) / 2$ & $(b + c) / 2$ \\
% {\em not-in-index} & $(b + c) / 2$ & $(a + d) / 2$ \\
% \end{tabular}
% \end{figure}

% An issue arises when evaluating the size of $P_{neg}$ in the
% comparison between an algorithm and a human indexer. Consider the
% sentence ``One of them is what's called the inner join on a
% condition.'' Say, the indexer classified the 2-gram {\em inner join}
% as belonging into the index, whereas the algorithm decided that
% nothing in this sentence should be included in the index. Do we now
% say that $P_{neg}$ is computed from all the possible phrase-level
% parts of the sentence that both indexer and algorithm decided not to
% place in the index? This decision would mean that all of the following
% should count towards boosting $P_{neg}$: ``One,'', ``One of,'' ``One
% of them,'' ``One of them is,'' and so on for n-grams of increasing
% $n$.
%
% Clearly this approach would make $P_{neg}$ meaningless. Instead we
% include in the set of candidates for consideration in the computation
% of $P_{neg}$ only 1-grams, and any n-grams that were chosen to be in
% the index by at least one rater.

Kappa values do not have a universally agreed upon interpretation, but
values in the range we observe (about 0.15 to 0.3) have been
interpreted as indicating ``slight'' to ``fair'' agreement. We measured an
agreement of 0.325 between humans in the gold index, suggesting that the phrase extraction
task is inherently subjective, and there are multiple valid
interpretations of what phrases are important enough to be included in
the index.

% \begin{figure*}
% \caption{}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
% \hline
% Document Boosting & 0.180 & 0.199 & 0.974 & 0.901 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
% \hline
% Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.204} & \textbf{0.224} & \textbf{0.975} & \textbf{0.902} \\
% \hline
% \end{tabular}
% \end{figure*}


\begin{figure*}[!ht]
\caption{}
\label{fig:main_result}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
\hline
TF-IDF & 0.205 & 0.233 & 0.971 & 0.889 \\
\hline
TF-IDF with Adjective-Noun Chunks & 0.079 & 0.118 & 0.961 & 0.850 \\
\hline
Document Boosting & 0.209 & 0.234 & 0.973 & 0.895 \\
\hline
Document Boosting with Adjective-Noun Chunks & 0.142 & 0.173 & 0.968 & 0.876 \\
\hline
Phrase Boosting & 0.204 & 0.234 & 0.970 & 0.883 \\
\hline
Phrase Boosting N-Grams & \textbf{0.237} & \textbf{0.262} & \textbf{0.974} & \textbf{0.899} \\
\hline
\end{tabular}
\end{figure*}

The metrics for all of the algorithms are shown in
Figure~\ref{fig:main_result}. The Phrase Boosting N-Grams algorithm,
which favors longer words, performed the best out of all algorithms,
and had a Cohen's Kappa of 0.237 agreement with the gold index. This nears the
lowest pairwise agreement between humans of 0.309, showing that the
algorithm is close to the performance of a human.Document Boosting with adjective-noun chunks seems to
yield significant improvement. Document Boosting and Phrase Boosting, the two algorithms that
incorporated external knowledge, were able to make improvements on the
basic algorithm. Document Boosting, which appended a Wikipedia document to
each lecture, was able to improve over TF-IDF, and boosting
longer phrases (Phrase Boosting N-Grams) was able to improve
further. 
% Interestingly, the algorithms seemed to be significantly closer to the
% indexer who took at least one database course in another institution.
% For example, the Document Boosting algorithm had a $\kappa$ of 0.240 and
% a $P_{\text{pos}}$ of 0.247 when compared to this indexer, while the
% $\kappa$s compared to other indexers were 0.146 and 0.191, and the
% $P_{\text{pos}}$ values were 0.147 and 0.147.

\begin{figure}[h]
\caption{The top 15 keywords from `Materialized Views' by Phrase
  Boosting with N-grams. Phrases that also appear in the gold index are marked in bold.}
\label{fig:top_15}
\begin{tabular}{|l|l|}
\hline
Rank & Phrase \\
\hline
1 & \textbf{view} \\
\hline
2 & \textbf{materialized view} \\
\hline
3 & materialized \\
\hline
4 & \textbf{query} \\
\hline
5 & \textbf{view query} \\
\hline
6 & \textbf{virtual view} \\
\hline
7 & \textbf{modify} \\
\hline
8 & user query \\
\hline
9 & \textbf{base table} \\
\hline
10 & \textbf{modify command} \\
\hline
11 & \textbf{index} \\
\hline
12 & insert command \\
\hline
13 & multivalued dependency \\
\hline
14 & \textbf{database design} \\
\hline
15 & user \\
\hline
\end{tabular}
\end{figure}

To give a more subjective view of our results, we also show the set of keywords extracted from a lecture on `Materialized Views' by the Phrase Boosting with N-grams algorithm, in Figure~\ref{fig:top_15} . Of the top 15 keywords marked by the algorithm, 11 were included in the gold index marked by humans (for this lecture there were 18 keywords in the gold set), and the algorithm produces a ranking that is similar to the humans. Of the keywords ranked highly by the algorithm that were not in the gold index, some (`materialized', `insert command', `multivalued dependency') are relevant to the course, but perhaps not essential to the specific lecture. The last two keywords, `user' and `user query' expose a weakness of the algorithm, where it is difficult to discern phrases that are used frequently, but not essential to the lecture concept.
