\subsection{Summary of Results}
\label{sec:sum}

% Here is where the comparisons between the two/three experimental
% results happens.


\begin{figure*}[!ht]
\caption{}
\label{fig:main_result}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
\hline
TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
\hline
TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
\hline
Document Boosting & 0.180 & 0.199 & 0.974 & 0.901 \\
\hline
Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
\hline
Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
\hline
Phrase Boosting N-Grams & \textbf{0.204} & \textbf{0.224} & \textbf{0.975} & \textbf{0.902} \\
\hline
\end{tabular}
\end{figure*}

As stated in section \ref{sec:gold}, we computed agreement between humans using Fleiss' Kappa. We evaluate each algorithm by computing Cohen's Kappa agreement between the algorithm and the union of the lecture-phrase pairs in the gold index, i.e. a lecture-phrase pair is classified as ``keyword'' if any human rater classified it as a keyword, and classified as ``not keyword'' otherwise. When using a single metric for agreement, such as Cohen's Kappa, paradoxes can arise where one category is much more prevalent than another and the chance-correction term in the agreement metric overcompensates, leading to low values of agreement \cite{feinstein1990high}. In our case, because there are many more possible phrases than keywords in a lecture, most phrases are calculated as ``not keyword''. To resolve this paradox, we report three additional metrics: agreement on keywords ($P_{\text{pos}}$), agreement on other phrases ($P_{\text{neg}}$) \cite{cicchetti1990high}, and prevalence-adjusted bias-adjusted kappa (PABAK) \cite{byrt1993bias}. Because the algorithms produce a ranking of candidate keywords, we produce a binary classification by choosing a threshold above which candidates are labeled as ``keyword''. In our reported results, we use the average number of keywords per lecture labeled by humans as the threshold.

These three metrics are defined in terms of a concordance table with two raters (the human and the algorithm) and two categories (``keyword'' and ``not keyword'')

\begin{figure}[h]
\caption{The concordance table used to calculate $P_{\text{pos}}$ and $P_{\text{neg}}$. }
\label{fig:concordance_1}
\begin{tabular}{l || c c c}
& Keyword & Not Keyword & Total \\
\hline \hline
Keyword & $a$ & $b$ & $f_1$ \\
Not Keyword & $c$ & $d$ & $f_2$ \\
Total & $g_1$ & $g_2$ & $N$ \\
\end{tabular}
\end{figure}

Positive agreement is the number of terms both marked as ``keyword'' over the average number of terms marked as ``keyword'' and negative agreement is the number of terms both marked as ``not keyword'' over the average number of terms marked as ``not keyword''
\begin{equation*}
P_{\text{pos}} \coloneqq \frac{a}{\frac{f_1 + g_1}{2}}
 \qquad P_{\text{neg}} \coloneqq \frac{d}{\frac{f_2 + g_2}{2}}
\end{equation*}

PABAK is defined as the Kappa on a modified concordance table where $a$ and $d$ are replaced with their average and $c$ and $d$ are replaced with their average

\begin{figure}[h]
\caption{The concordance table used to calculate PABAK. $a$, $b$, $c$, and $d$ are as defined in Figure \ref{fig:concordance_1}}
\begin{tabular}{l || c c }
& Keyword & Not Keyword \\
\hline \hline
Keyword & $(a + d) / 2$ & $(b + c) / 2$ \\
Not Keyword & $(b + c) / 2$ & $(a + d) / 2$ \\
\end{tabular}
\end{figure}

Kappa values do not have a universally agreed upon interpretation, but values in the range we observe (about 0.15 to 0.3) have been interpreted as indicating ``slight'' to ``fair'' agreement. The fact that agreement between humans is only 0.336, and the lowest pairwise Kappa between humans was 0.309 suggests that the keyword extraction task is inherently subjective, and there are multiple valid interpretations of what phrases are important enough to be attached to a document.

The vanilla TF-IDF algorithm was already able to achieve reasonable performance, with respect to the human annotators. Limiting the candidate set to adjective-noun chunks drastically hurt the performance of the algorithm, suggesting that many important phrases do not fit this linguistic pattern, and the restriction is too severe. Document Boosting and Phrase Boosting, the two algorithms that incorporated external knowledge, were able to make improvements on the basic algorithm. Phrase Boosting with the TF-IDF scores of all candidate keywords was slightly better than TF-IDF, and only boosting longer phrases (Phrase Boosting N-Grams) was able to improve further. We can see that negative agreement was high for all algorithms, suggesting that it is relatively easy to identify phrases that shouldn't be keywords. Positive agreement was lower, implying it is indeed difficult to identify a small number of phrases to summarize the document, given that there are a large number of possible phrases, and there can be legitimate disagreement on what phrases are most important. Note that PABAK is not linearly related to Cohen's Kappa, $P_{\text{pos}}$, and $P_{\text{neg}}$, which explains how the Document Boosting algorithm can have a higher PABAK despite having a lower $P_{\text{pos}}$ and Cohen's Kappa.
