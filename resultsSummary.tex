\subsection{Results}
\label{sec:sum}

% Here is where the comparisons between the two/three experimental
% results happens.


% \begin{figure*}
% \caption{}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
% \hline
% Document Boosting & 0.180 & 0.199 & 0.974 & 0.901 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
% \hline
% Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.204} & \textbf{0.224} & \textbf{0.975} & \textbf{0.902} \\
% \hline
% \end{tabular}
% \end{figure*}

% \begin{figure*}[!ht]
% \caption{Result without the third rater}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.205 & 0.233 & 0.971 & 0.889 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.079 & 0.118 & 0.961 & 0.850 \\
% \hline
% Document Boosting & 0.209 & 0.234 & 0.973 & 0.895 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.142 & 0.173 & 0.968 & 0.876 \\
% \hline
% Phrase Boosting & 0.204 & 0.234 & 0.970 & 0.883 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.237} & \textbf{0.262} & \textbf{0.974} & \textbf{0.899} \\
% \hline
% \end{tabular}
% \end{figure*}

% \begin{figure*}[!ht]
% \caption{Result with only the top 5}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.227 & 0.241 & 0.986 & 0.946 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.108 & 0.125 & 0.982 & 0.931 \\
% \hline
% Document Boosting & 0.237 & 0.250 & 0.988 & 0.951 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.148 & 0.163 & 0.985 & 0.942 \\
% \hline
% Phrase Boosting & 0.223 & 0.237 & 0.986 & 0.945 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.253} & \textbf{0.265} & \textbf{0.987} & \textbf{0.950} \\
% \hline
% \end{tabular}
% \end{figure*}

We evaluated each algorithm by computing Cohen's Kappa agreement
between the algorithm and a gold set created by unifying two of the
human indexes\footnote{One of the human indexes was excluded because
  it sometimes included words that did not appear in the lecture.}.
We chose a widely employed inter-rater reliability measure because
indexing is highly subjective. Given this absence of absolute truth
we therefore treated the algorithms as we would have measured
reliability of an additional human indexer.

%
% This problem arises in our context because there are many more {\em not-in-index} phrases than
% phrases destined for the index. To help deconstruct the above-mentioned
% inappropriate $\kappa$ values, we report three additional metrics:
% agreement on {\em in-index} decisions ($P_{\text{pos}}$), agreement on
% {\em not-in-index} decisions ($P_{\text{neg}}$)
% \cite{cicchetti1990high}, and prevalence-adjusted bias-adjusted
% $\kappa$ (PABAK) \cite{byrt1993bias}. Because the algorithms produce a
% ranking of candidate phrases, we produce a binary classification by
% choosing a threshold above which candidates are labeled as {\em
%   in-index}. In our reported results, we use the average number of
% keywords per lecture labeled by humans as the threshold.
%
% These three metrics are defined in terms of a concordance table with
% two raters (the composite-human and the algorithm) and two
% categories ({\em in-index} and {\em not-in-index}), as shown in
% Figure~\ref{fig:concordance_1} and Figure~\ref{fig:pabak}.

% \begin{figure}[h]
% \caption{The concordance table used to calculate $P_{\text{pos}}$ and $P_{\text{neg}}$. }
% \label{fig:concordance_1}
% \begin{tabular}{l || c c c}
% & {\em in-index} & {\em not-in-index} & Total \\
% \hline \hline
% Keyword & $a$ & $b$ & $f_1$ \\
% Not Keyword & $c$ & $d$ & $f_2$ \\
% Total & $g_1$ & $g_2$ & $N$ \\
% \end{tabular}
% \end{figure}

% Positive agreement is the number of terms both indexers marked as {\em
%   in-index} over the average number of terms marked as {\em in-index}, and
% negative agreement is the number of terms both indexers marked as {\em not
%   in-index} over the average number of terms marked as {\em not
%   in-index}:
% \begin{equation*}
% P_{\text{pos}} \coloneqq \frac{a}{\frac{f_1 + g_1}{2}}
%  \qquad P_{\text{neg}} \coloneqq \frac{d}{\frac{f_2 + g_2}{2}}
% \end{equation*}
%
% PABAK is defined as the Kappa on a modified concordance table where $a$ and $d$ are replaced with their average, and $c$ and $d$ are replaced with their average.
%
% \begin{figure}[h]
% \caption{The concordance table used to calculate PABAK. $a$, $b$, $c$, and $d$ are as defined in Figure \ref{fig:concordance_1}}
% \label{fig:pabak}
% \begin{tabular}{l || c c }
% & {\em in-index} & {\em not-in-index} \\
% \hline \hline
% {\em in-index} & $(a + d) / 2$ & $(b + c) / 2$ \\
% {\em not-in-index} & $(b + c) / 2$ & $(a + d) / 2$ \\
% \end{tabular}
% \end{figure}

% An issue arises when evaluating the size of $P_{neg}$ in the
% comparison between an algorithm and a human indexer. Consider the
% sentence ``One of them is what's called the inner join on a
% condition.'' Say, the indexer classified the 2-gram {\em inner join}
% as belonging into the index, whereas the algorithm decided that
% nothing in this sentence should be included in the index. Do we now
% say that $P_{neg}$ is computed from all the possible phrase-level
% parts of the sentence that both indexer and algorithm decided not to
% place in the index? This decision would mean that all of the following
% should count towards boosting $P_{neg}$: ``One,'', ``One of,'' ``One
% of them,'' ``One of them is,'' and so on for n-grams of increasing
% $n$.
%
% Clearly this approach would make $P_{neg}$ meaningless. Instead we
% include in the set of candidates for consideration in the computation
% of $P_{neg}$ only 1-grams, and any n-grams that were chosen to be in
% the index by at least one rater.

Kappa values do not have a universally agreed upon interpretation, but
values in the range we observe (about 0.15 to 0.3) have been
interpreted as indicating ``slight'' to ``fair'' agreement. We measured
agreement of 0.325 between the humans in the gold index. This value is
therefore the measure to beat.

% \begin{figure*}
% \caption{}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
% \hline
% Document Boosting & 0.180 & 0.199 & 0.974 & 0.901 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
% \hline
% Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.204} & \textbf{0.224} & \textbf{0.975} & \textbf{0.902} \\
% \hline
% \end{tabular}
% \end{figure*}

\begin{figure}[!ht]
\caption{}
\label{fig:main_result}
\begin{tabular}{|c|c|}
\hline
\textbf{Algorithm} & $\mathbf{\kappa}$ \\
\hline
TF-IDF & 0.205 \\
\hline
TF-IDF with Adjective-Noun Chunks & 0.079 \\
\hline
Document Boosting & 0.209 \\
\hline
Document Boosting with Adjective-Noun Chunks & 0.142 \\
\hline
Phrase Boosting & 0.204 \\
\hline
Phrase Boosting N-Grams & \textbf{0.237} \\
\hline
\end{tabular}
\end{figure}

The metrics for all of the algorithms are shown in
Figure~\ref{fig:main_result}. The Phrase Boosting N-Grams algorithm,
which favors longer words, performed best with a Cohen's Kappa of
0.237.

%% Document Boosting with adjective-noun chunks yielded
%% significant improvement. Document Boosting and Phrase Boosting, the
%% two algorithms that incorporated Wikipedia, improved on the basic
%% algorithm. Document Boosting improved over TF-IDF, and boosting longer
%% phrases (Phrase Boosting N-Grams) yielded higher values still.

% Interestingly, the algorithms seemed to be significantly closer to the
% indexer who took at least one database course in another institution.
% For example, the Document Boosting algorithm had a $\kappa$ of 0.240 and
% a $P_{\text{pos}}$ of 0.247 when compared to this indexer, while the
% $\kappa$s compared to other indexers were 0.146 and 0.191, and the
% $P_{\text{pos}}$ values were 0.147 and 0.147.

Figure~\ref{fig:top_15} shows the set of keywords extracted from a
lecture on `Materialized Views' by the Phrase Boosting with N-grams
algorithm, in Figure~\ref{fig:top_15} . Of the top 15 keywords marked
by the algorithm, 11 were included in the gold index marked by humans
(for this lecture there were 18 keywords in the gold set), and the
algorithm produces a ranking that is similar to the humans. Of the
keywords ranked highly by the algorithm that were not in the gold
index, some (`materialized', `insert command', `multivalued
dependency') are relevant to the course, but perhaps not essential to
the specific lecture. The last two keywords, `user' and `user query'
expose a weakness of the algorithm, where it is difficult to discern
phrases that are used frequently, but not essential to the lecture
concept.
