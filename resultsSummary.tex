\subsection{Summary of Results}
\label{sec:sum}

% Here is where the comparisons between the two/three experimental
% results happens.


\begin{figure}
\caption{The top 15 keywords from `Materialized Views' by Phrase Boosting with N-grams. Words in the gold index are marked in bold.}
\label{fig:top_15}
\begin{tabular}{|l|l|}
\hline
Rank & Phrase \\
\hline
1 & \textbf{view} \\
\hline
2 & \textbf{materialized view} \\
\hline
3 & materialized \\
\hline
4 & \textbf{query} \\
\hline
5 & \textbf{view query} \\
\hline
6 & \textbf{virtual view} \\
\hline
7 & \textbf{modify} \\
\hline
8 & user query \\
\hline
9 & \textbf{base table} \\
\hline
10 & \textbf{modify command} \\
\hline
11 & \textbf{index} \\
\hline
12 & insert command \\
\hline
13 & multivalued dependency \\
\hline
14 & \textbf{database design} \\
\hline
15 & user \\
\hline
\end{tabular}
\end{figure}


\begin{figure*}[!ht]
\caption{}
\label{fig:main_result}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
\hline
TF-IDF & 0.178 & 0.200 & 0.973 & 0.896 \\
\hline
TF-IDF with Adjective-Noun Chunks & 0.084 & 0.114 & 0.968 & 0.875 \\
\hline
Document Boosting & 0.180 & 0.199 & 0.974 & 0.901 \\
\hline
Document Boosting with Adjective-Noun Chunks & 0.131 & 0.154 & 0.971 & 0.890 \\
\hline
Phrase Boosting & 0.180 & 0.203 & 0.973 & 0.895 \\
\hline
Phrase Boosting N-Grams & \textbf{0.204} & \textbf{0.224} & \textbf{0.975} & \textbf{0.902} \\
\hline
\end{tabular}
\end{figure*}

% \begin{figure*}[!ht]
% \caption{Result without the third rater}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.205 & 0.233 & 0.971 & 0.889 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.079 & 0.118 & 0.961 & 0.850 \\
% \hline
% Document Boosting & 0.209 & 0.234 & 0.973 & 0.895 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.142 & 0.173 & 0.968 & 0.876 \\
% \hline
% Phrase Boosting & 0.204 & 0.234 & 0.970 & 0.883 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.237} & \textbf{0.262} & \textbf{0.974} & \textbf{0.899} \\
% \hline
% \end{tabular}
% \end{figure*}

% \begin{figure*}[!ht]
% \caption{Result with only the top 5}
% \label{fig:main_result}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Algorithm} & $\mathbf{\kappa}$ & $\mathbf{P_{\text{pos}}}$ & $\mathbf{P_{\text{neg}}}$ & \textbf{PABAK} \\
% \hline
% TF-IDF & 0.227 & 0.241 & 0.986 & 0.946 \\
% \hline
% TF-IDF with Adjective-Noun Chunks & 0.108 & 0.125 & 0.982 & 0.931 \\
% \hline
% Document Boosting & 0.237 & 0.250 & 0.988 & 0.951 \\
% \hline
% Document Boosting with Adjective-Noun Chunks & 0.148 & 0.163 & 0.985 & 0.942 \\
% \hline
% Phrase Boosting & 0.223 & 0.237 & 0.986 & 0.945 \\
% \hline
% Phrase Boosting N-Grams & \textbf{0.253} & \textbf{0.265} & \textbf{0.987} & \textbf{0.950} \\
% \hline
% \end{tabular}
% \end{figure*}

As stated in Section~\ref{sec:gold}, we computed agreement between
humans using Fleiss' Kappa. We evaluated each algorithm by computing
Cohen's Kappa agreement between the algorithm and the gold set
unified from the three human indexes as described in
Section~\ref{sec:gold}: the union of the lecture-phrase pairs in the
gold indexes. That is a lecture-phrase pair is classified as {\em
  in-index} if any human indexer classified it as {\em in-index}, and
classified as {\em not-in-index} otherwise. When using a single metric
for agreement, such as Cohen's Kappa, paradoxes can arise when one
category is much more prevalent than another and the chance-correction
term in the agreement metric overcompensates. This effect leads to
inappropriately low values of $\kappa$ for high inter-rater agreement
\cite{feinstein1990high}.

In our case, there are many more {\em not-in-index} phrases than
phrases destined for the index. To illuminate the above-mentioned
inappropriate $\kappa$ values, we report three additional metrics:
agreement on {\em in-index} decisions ($P_{\text{pos}}$), agreement on
{\em not-in-index} decisions ($P_{\text{neg}}$)
\cite{cicchetti1990high}, and prevalence-adjusted bias-adjusted
$\kappa$ (PABAK) \cite{byrt1993bias}. Because the algorithms produce a
ranking of candidate phrases, we produce a binary classification by
choosing a threshold above which candidates are labeled as {\em
  in-index}. In our reported results, we use the average number of
keywords per lecture labeled by humans as the threshold.

These three metrics are defined in terms of a concordance table with
two raters (the composite-human and the algorithm) and two
categories ({\em in-index} and {\em not-in-index})

\begin{figure}[h]
\caption{The concordance table used to calculate $P_{\text{pos}}$ and $P_{\text{neg}}$. }
\label{fig:concordance_1}
\begin{tabular}{l || c c c}
& {\em in-index} & {\em not-in-index} & Total \\
\hline \hline
Keyword & $a$ & $b$ & $f_1$ \\
Not Keyword & $c$ & $d$ & $f_2$ \\
Total & $g_1$ & $g_2$ & $N$ \\
\end{tabular}
\end{figure}

Positive agreement is the number of terms both marked as {\em
  in-index} over the average number of terms marked as {\em in-index} and
negative agreement is the number of terms both marked as {\em not
  in-index} over the average number of terms marked as {\em not
  in-index}
\begin{equation*}
P_{\text{pos}} \coloneqq \frac{a}{\frac{f_1 + g_1}{2}}
 \qquad P_{\text{neg}} \coloneqq \frac{d}{\frac{f_2 + g_2}{2}}
\end{equation*}

PABAK is defined as the Kappa on a modified concordance table where $a$ and $d$ are replaced with their average and $c$ and $d$ are replaced with their average

\begin{figure}[h]
\caption{The concordance table used to calculate PABAK. $a$, $b$, $c$, and $d$ are as defined in Figure \ref{fig:concordance_1}}
\begin{tabular}{l || c c }
& {\em in-index} & {\em not-in-index} \\
\hline \hline
{\em in-index} & $(a + d) / 2$ & $(b + c) / 2$ \\
{\em not-in-index} & $(b + c) / 2$ & $(a + d) / 2$ \\
\end{tabular}
\end{figure}

Kappa values do not have a universally agreed upon interpretation, but
values in the range we observe (about 0.15 to 0.3) have been
interpreted as indicating ``slight'' to ``fair'' agreement. The fact
that agreement between humans is only 0.336, and the lowest pairwise
Kappa between humans was 0.309 suggests that the phrase extraction
task is inherently subjective, and there are multiple valid
interpretations of what phrases are important enough to be included in
the index.

The plain TF-IDF algorithm was already able to achieve reasonable
performance, with respect to the human annotators. Limiting the
candidate set to adjective-noun chunks drastically hurt the
performance of the algorithm, suggesting that many important phrases
do not fit this linguistic pattern, and the restriction is too
severe. However, Document Boosting with adjective-noun chunks seems to
yield significant improvement. Document Boosting and Phrase Boosting, the two algorithms that
incorporated external knowledge, were able to make improvements on the
basic algorithm. Phrase Boosting with the TF-IDF scores of all
candidate phrases was slightly better than TF-IDF, and only boosting
longer phrases (Phrase Boosting N-Grams) was able to improve
further. We can see that negative agreement was high for all
algorithms, suggesting that it is relatively easy to identify phrases
that should not be indexed. Positive agreement was lower, implying it is indeed difficult to identify a small number of phrases to summarize the document, given that there are a large number of possible phrases, and there can be legitimate disagreement on what phrases are most important. Note that PABAK is not linearly related to Cohen's Kappa, $P_{\text{pos}}$, and $P_{\text{neg}}$, which explains how the Document Boosting algorithm can have a higher PABAK despite having a lower $P_{\text{pos}}$ and Cohen's Kappa.

Interestingly, the algorithms seemed to be significantly closer to the
indexer who took at least one database course in another institution.
For example, the Document Boosting algorithm had a $\kappa$ of 0.240 and
a $P_{\text{pos}}$ of 0.247 when compared to this indexer, while the
$\kappa$s compared to other indexers were 0.146 and 0.191, and the
$P_{\text{pos}}$ values were 0.147 and 0.147.

To give a more subjective view of our results, we also show the set of keywords extracted from a lecture on `Materialized Views' by the Phrase Boosting with N-grams algorithm, in Figure~\ref{fig:top_15} . Of the top 15 keywords marked by the algorithm, 11 were included in the gold index marked by humans (for this lecture there were 18 keywords in the gold set), and the algorithm produces a ranking that is similar to the humans. Of the keywords ranked highly by the algorithm that were not in the gold index, some (`materialized', `insert command', `multivalued dependency') are relevant to the course, but perhaps not essential to the specific lecture. The last two keywords, `user' and `user query' expose a weakness of the algorithm, where it is difficult to discern phrases that are used frequently, but not essential to the lecture concept.
