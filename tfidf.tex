\subsection{Traditional Approach: TF-IDF}
\label{sec:tfidf}

% Simply discard sequencing information of terms occurrences. For each
% video put the top-n tf/idf terms into the index \cite{mann2008}.

Our simplest algorithm used a straight term frequency-inverse document
frequency (TF-IDF) approach to identifying index terms in a
lecture. TF-IDF is defined for each phrase-lecture pair as the product
of the number of times the phrase appears in the lecture, divided by
the logarithm of the proportion of lectures in which the phrase
appears. That is, we used the standard definition of TF-IDF, with each
lecture taken as a document. We then ranked the phrases for each
document by their TF-IDF score in that document. In the simplest case
we used the average number of keywords per lecture as evidenced in the
humans' indexes as the threshold.

% Formally, the term frequency $TF(p, d)$ for a phrase $p$ and lecture $l$ is defined as
% \begin{equation*}
% TF(p, l) \coloneqq \text{number of times } p \text{ appears in } l
% \end{equation*}
% and the $IDF(p, L)$ for a phrase $p$ and collection of lectures $L$ is defined as
% \begin{equation*}
% IDF(p, L) \coloneqq \log\left(\frac{|L|}{\text{number of documents in } L \text{ with } p} + 1\right)
% \end{equation*}
% Where a 1 is added as a heuristic so that terms appearing in all documents are not entirely discarded. Finally, $TF\text{-}IDF(p, l, L)$ is defined as
% \begin{equation*}
% TF\text{-}IDF(p, l, L) \coloneqq TF(p, l) \cdot IDF(p, L)
% \end{equation*}

% The algorithm computes $TF\text{-}IDF(p, l, L)$ for every phrase-lecture pair in $L$, and then ranks the phrases for each lecture by their TF-IDF.
